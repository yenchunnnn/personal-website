---
title: Classifying depressed and control groups by motor activity time series data
author: Yen-Chun Chen
date: '2024-02-16'
slug: classifying-depressed-and-healthy-controls
categories:
  - R
  - Mechine Learning
tags:
  - Data Science
  - Classification
  - R Programming
  - Time Series
  - Clinical Psychology
subtitle: ''
summary: 'Applying logistic regression, random forest, and SVM to analyze the daily motor activity patterns of participants.'
authors: []
lastmod: '2024-03-11T12:59:04+08:00'
featured: no
image:
  caption: 'Friends scenes from NBC'
  focal_point: 'smart'
  preview_only: no
projects: []
output:
  blogdown::html_page:
    toc: true
---

## Introduction

Depressive disorders is a cluster of mental disorders characterized by the presence of sad, empty, or irritable mood, accompanied by somatic and cognitive changes that significantly affect the individual's capacity to function. [^1]

[^1]: American Psychiatric Association. (2013). Diagnostic and statistical manual of mental disorders (5th ed.). <https://doi.org/10.1176/appi.books.9780890425596>

The traditional practice of assessing depressed mood states largely depends on subjective reports or observations made by others, combined with structured clinical rating scales. However, subjectiveness has the potential to lead to bias and misdiagnosis, which will affect the recovery and subsequent treatment as well as intervention. Therefore, when making a clinical diagnosis, several objective indicators may be taken into account.

Alteration of motor activity (psychomotor) is one of the diagnostic criteria in depressive disorders. The depressive state is often associated with lower mean activity, higher intraindividual variability, and less complexity in activity patterns when compared to healthy controls. [^2] [^3] And wearable technology-based ecological momentary assessment allows us to collect objective and real-time data on participants' daily motor activity.

[^2]: Burton, C., McKinstry, B., Szentagotai Tătar, A., Serrano-Blanco, A., Pagliari, C., & Wolters, M. (2013). Activity monitoring in patients with depression: a systematic review. *Journal of affective disorders, 145(1)*, 21--28. <https://doi.org/10.1016/j.jad.2012.07.001>

[^3]: Krane-Gartiser, K., Henriksen, T. E., Vaaler, A. E., Fasmer, O. B., & Morken, G. (2015). Actigraphically assessed activity in unipolar depression: a comparison of inpatients with and without motor retardation. *The Journal of clinical psychiatry, 76(9)*, 1181--1187. <https://doi.org/10.4088/JCP.14m09106>

So the aim of this project was to investigate whether objective biological metrics may improve on traditional diagnosis procedures by analyzing motor activity patterns in both healthy and depressed participants using machine learning techniques.

## Preprocessing

```{r message=FALSE, warning=FALSE}
# Library package
library(tidyverse)
library(zoo)
library(xts)
library(rsample)
library(caret)
library(vip)
library(tsfeatures)
```

### Setting up dataframe

**Data description**

The Depresjon dataset is a collection of data that includes the motor activity of participants measured with a wrist-worn actigraph.[^4] The sampling frequency is 32Hz and movements greater than 0.05g are recorded. The number of counts corresponds to the intensity of the movement. Total activity counts were continually recorded at one-minute intervals. The following files contain:

[^4]: Garcia-Ceja, E., Riegler, M., Jakobsen, P., Torresen, J., Nordgreen, T., Oedegaard, K. J., & Fasmer, O. B. (2018). DEPRESJON dataset [Data set]. Zenodo. <https://doi.org/10.5281/zenodo.1219550>

``` markmap{height="200px"}
- Depresjon dataset
  - Condition: 23 depressed participants
    - timestamp (one minute intervals)
    - date (date of measurement): inconsistency for every participant.
    - activity
  - Control: 32 healthy participants
    - timestamp (one minute intervals)
    - date (date of measurement): inconsistency for every participant.
    - activity
  - Scores: all participants
    - Demographic
    - Severity of depression
```

We only used actigraph data in this analysis.

```{r include=FALSE}
condition_path <- 'C:/Users/User/Desktop/研究所課程 隨便看/機器學習/The depression dataset/condition'

control_path <- 'C:/Users/User/Desktop/研究所課程 隨便看/機器學習/The depression dataset/control'

# Change to UTC timezone
Sys.setlocale("LC_TIME", "C")
```

```{r}
# Load actigraph files

# condition files
condition_file <- list.files(path = condition_path, pattern = '*.csv', full.names = T)

# control files
control_file <- list.files(path = control_path, pattern = '*.csv', full.names = T)

# number of files
c(condition = length(condition_file), control = length(control_file))
```

Create a condition data frame.

```{r message=FALSE}
# read condition files
condition_data <- read_csv(condition_file, id = "ID") %>%
    mutate(ID = parse_number(ID),
           timestamp = as.POSIXct(str_replace_all(timestamp, "/", "-")),
           date = as_date(str_replace_all(date, "/", "-")),
           group = 'condition')
glimpse(condition_data)
```

Create a control data frame.

```{r message=FALSE}
# read control files
control_data <- read_csv(control_file, id = "ID") %>%
    mutate(ID = parse_number(ID) + 23,
           group = 'control')
glimpse(control_data)
```

Merge two data frames.

```{r}
# combine control and condition data
full_df <- rbind(condition_data, control_data) %>%
    mutate(group = as.factor(group))

head(full_df)
```

Notice that this is a multilevel data frame:

-   Level 5: group

-   Level 4: subject

-   Level 3: date

-   Level 2: hour

-   Level 1: minute

### Cleaning data

Check if there is any missing data.

```{r}
# check missing data
sum(is.na(full_df))
```

Visualize the data to see overall patterns. Black line was the original time series, red line was the mean activity in every 30 minutes.

```{r}
# nest by ID
nest_df <- full_df %>%
    group_by(ID) %>%
    nest() %>%
    arrange(ID)

# plot function: plotting original & aggregated time series
plot_fun <- function(zoo, zoo_agg) {
    ggplot(zoo, aes(Index, zoo)) + 
        geom_line() + 
        scale_y_continuous() +
        # add a line plot for the weekly aggregated time series
        geom_line(data = zoo_agg, aes(Index, zoo_agg),
                   # color the aggregated line in red
                  color = "red") +
        theme(axis.title = element_blank(),
              axis.text.x = element_blank())
}

# create plot columns
nest_df <- nest_df %>%
    mutate(
        # create zoo object
        zoo = map(data, ~zoo(x = .x[["activity"]], order.by = .x[["timestamp"]])),
        # create the index from every 30 mins
        hour_index = map(zoo, ~endpoints(.x, on = "minutes", k = 30)),
        # apply the mean to the time series using the index
        zoo_agg = map2(zoo, hour_index, ~period.apply(.x, INDEX = .y, FUN = mean)),
        # plot time series
        plot = map2(zoo, zoo_agg, plot_fun))

# condition plots
grid.arrange(grobs = nest_df$plot[1:23], ncol = 4)

# control plots, cause it's too many plots if we put all of them in one figure,
# we'll separate plotting them
grid.arrange(grobs = nest_df$plot[24:39], ncol = 4)
grid.arrange(grobs = nest_df$plot[40:55], ncol = 4)
```

It appears that there were days when subjects did not record activities; thus, we must delete those days. Mean activity of a day that lower than the threshold (threshold = 25) was removed. In this sense, we should preserve within-day variability.

```{r}
# filter days where doesn't fit threshold
clean_df <- full_df %>%
    group_by(ID, date) %>%
    mutate(mean_perday = mean(activity)) %>%
    # mean activity per day threshold: 25
    filter(mean_perday > 25)

# see the rows difference
list(full = dim(full_df), clean_subact = dim(clean_df))
```

There may be some days that's not a record for 24 hours, which means fewer than 24 × 60 = 1440 minutes.

```{r}
# check if there any day that's < 1440 mins
clean_df %>% 
    group_by(ID, date) %>%
    count() %>%
    filter(n < 1440)
```

```{r}
# remove the rows that's not completed
clean_df <- clean_df %>%
    group_by(ID, date) %>%
    mutate(minutes = length(timestamp)) %>%
    filter(minutes == 1440)

# see the difference
list(full = dim(full_df), clean_comact = dim(clean_df))
```

## Exploratory Data Analysis

### Day between groups

```{r}
# summary of the days between groups
day_df <- clean_df %>% 
    group_by(ID) %>% 
    summarise(day = length(unique(date))) %>%
    mutate(group = as.factor(ifelse(ID <= 23, 'condition', 'control')))

psych::describeBy(day_df$day, group = day_df$group)
```

```{r}
# sum of days
day_df %>% 
    group_by(group) %>%
    summarise(count = sum(day))
```

|       | Condition | Control |
|-------|-----------|---------|
| total | 329       | 463     |
| mean  | 14.30     | 14.47   |
| sd    | 1.79      | 1.41    |
| min   | 10        | 13      |
| max   | 19        | 21      |

: Descriptive statistics of collected days by group

### Activity between groups

#### Overall 24-hour span

We primarily chose three statistical features that were extracted from the *date level* in this step.[^5] [^6]

[^5]: Garcia-Ceja, E., Riegler, M., Jakobsen, P., Tørresen, J., Nordgreen, T., Oedegaard, K.J., & Fasmer, O.B. (2018). Depresjon: a motor activity database of depression episodes in unipolar and bipolar patients. *Proceedings of the 9th ACM Multimedia Systems Conference.*

[^6]: Jakobsen, P., Garcia-Ceja, E., Riegler, M., Stabell, L. A., Nordgreen, T., Torresen, J., Fasmer, O. B., & Oedegaard, K. J. (2020). Applying machine learning in motor activity time series of depressed bipolar and unipolar patients compared to healthy controls. *PloS one, 15(8),* e0231995. <https://doi.org/10.1371/journal.pone.0231995>

Which are:

-   Mean activity

-   Standard deviation of activity

-   The proportion of minutes with an activity level of zero within a day

Based on previous studies, we may hypothesize that the condition group would have *lower mean activity*, a *higher variability of activity*, and a *higher zero activity of proportion* in a day compared to the control group.

Because the activity isn't a normal distribution (e.g., condition group would have more low activity levels than high activity levels), we have to normalize it before further calculation. Here we used log transformation to adjust skewness.

```{r}
# calculate mean activity after log transform
clean_df <- clean_df %>% 
    group_by(ID, date) %>% 
    mutate(log_activity = log(activity + 1),
           mean_activity = mean(log_activity),
           sd_activity = sd(log_activity), 
           zero_prop = zero_proportion(log_activity))

# select needed columns and filter rows
group_act <- clean_df %>%
    select(ID, group, date, mean_activity, sd_activity, zero_prop) %>%
    distinct() 

# box plot for this three vars
reshape2::melt(group_act[, c(2, 4:6)], id.vars = "group") %>%
    # Everything on the same plot
    ggplot(., aes(group, value, col = variable)) + 
      geom_boxplot()
```

It seems like there's still some noisy (outlier) activity, even though we've already log transformed data.

```{r}
# summarise group activity
group_act %>%
    group_by(group) %>%
    summarise(mean = mean(mean_activity),
              sd = mean(sd_activity),
              zero_prop = mean(zero_prop))
```

The descriptive statistics make sense; expect for the variability. Let's see if the differences are significant or not.

```{r}
# test for homogeneity in variances
car::leveneTest(mean_activity ~ group, data = group_act)
car::leveneTest(sd_activity ~ group, data = group_act)
car::leveneTest(zero_prop ~ group, data = group_act)
```

Since there are inhomogeneous variances, we have to set `var.equal = FALSE` for mean and zero proportion and `var.equal = TRUE` for standard deviation.

```{r}
# corresponding two sample t-test

# mean: condition less than control
t.test(mean_activity ~ group, data = group_act, alternative = "less", var.equal = FALSE)

# sd: condition greater than control
t.test(sd_activity ~ group, data = group_act, alternative = "greater", var.equal = TRUE)

# zero proportion: condition greater than control
t.test(zero_prop ~ group, data = group_act, alternative = "greater", var.equal = FALSE)
```

|                 | Condition | Control |             |
|-----------------|-----------|---------|-------------|
| mean            | 2.80      | 3.44    | *p \< .001* |
| sd              | 2.50      | 2.63    | *p = 1*     |
| zero proportion | 0.41      | 0.32    | *p \< .001* |

: Characteristics of the depressed patients and healthy controls in 24-hour activity

#### Under an hourly span

The activity was normalized (range: 0-1) across both groups to make them comparable. Here we'll use the heat map and line graph to compare activities between groups by hours.

```{r message=FALSE, warning=FALSE}
# add normal and hour columns
clean_df <- clean_df %>%
    mutate(normal_zerone = scales::rescale(log_activity),
           hour = hour(timestamp))

# heat map
heat_plot <- clean_df %>%
    group_by(group, hour) %>%
    summarise(activity = mean(normal_zerone)) %>%
    ggplot(., aes(group, hour, fill = activity)) + 
        geom_tile() +
        scale_fill_gradient(low = "white", high = "blue")

# line plot
line_plot <- clean_df %>%
    group_by(group, hour) %>%
    summarise(activity = mean(normal_zerone)) %>%
    ggplot(., aes(hour, activity, color = group)) +
        geom_line()

grid.arrange(heat_plot, line_plot, ncol = 1)
```

It's interesting that the control group has an opposite circadian rhythm as we thought. But we can see from the heat map that the condition group may have slightly lower activity compared to the control group.

## Modeling

### Feature extraction

Besides the mean, standard deviation, and zero proportion of activity, we extracted other features according to previous studies.[^7] Including:

[^7]: Zanella-Calzada, L. A., Galván-Tejada, C. E., Chávez-Lamas, N. M., Gracia-Cortés, M. D. C., Magallanes-Quintanar, R., Celaya-Padilla, J. M., Galván-Tejada, J. I., & Gamboa-Rosales, H. (2019). Feature Extraction in Motor Activity Signal: Towards a Depression Episodes Detection in Unipolar and Bipolar Patients. *Diagnostics (Basel, Switzerland), 9(1),* 8. <https://doi.org/10.3390/diagnostics9010008>

-   Coefficient of variation

-   Kurtosis

-   Skewness

-   Quantile 25, 75, 95, 99

-   Median

-   Number of times a time series crosses the median line

```{r message=FALSE, warning=FALSE}
# set up final features df for ML
feature_df <- clean_df %>%
    group_by(ID, date) %>%
    summarise(
        mean_act = mean(log_activity),
        sd_act = sd(log_activity), 
        zero_prop = zero_proportion(log_activity),
        # Coefficient of variation
        cv_act = sd_act / mean_act, 
        skew_act = psych::skew(log_activity),
        kurtosi_act = psych::kurtosi(log_activity),
        q99_act = quantile(log_activity, probs = 0.99),
        q95_act = quantile(log_activity, probs = 0.95),
        q75_act = quantile(log_activity, probs = 0.75),
        q25_act = quantile(log_activity, probs = 0.25),
        med_act = median(log_activity),
        #  number of times crosses the median line
        cross_medpoint = crossing_points(log_activity)
    ) %>%
    distinct() %>%
    mutate(group = as.factor(if_else(ID < 24, "condition", "control"))) %>%
    ungroup() %>%
    select(-c("ID", "date"))

feature_df
```

### Splitting train/test set

Create a single 70/30 split of the data.

```{r}
set.seed(123)

# split train/test
split_data <- initial_split(feature_df, prop = 0.7, strata = 'group')
train_set <- training(split_data)
test_set <- testing(split_data)

# see prop
list(train_num = table(train_set$group),
     train_prop = table(train_set$group) %>% prop.table(), 
     test_num = table(test_set$group),
     test = table(test_set$group) %>% prop.table())
```

### Training models

Baseline model: logistic regression

-   Traditional two classes classification model

Comparison model: random forest, linear SVM

-   Linear SVM: had a higher model performance in the past study [^8]

-   Random forest: easier to explain; high accuracy and superiority with imbalanced dataset [^9]

[^8]: Garcia-Ceja, E., Riegler, M., Jakobsen, P., Torresen, J., Nordgreen, T., Oedegaard, K. J., & Fasmer, O. B. (2018). DEPRESJON dataset [Data set]. Zenodo. <https://doi.org/10.5281/zenodo.1219550>

[^9]: A. S. More and D. P. Rana, "Review of random forest classification techniques to resolve data imbalance," *2017 1st International Conference on Intelligent Systems and Information Management (ICISIM),* Aurangabad, India, 2017, pp. 72-78, doi: 10.1109/ICISIM.2017.8122151.

Subsampling for class imbalances

-   down-sampling

-   up-sampling

-   hybrid methods: SMOTE

So we'll have a total of 3 × 4 = 12 models in comparison.

The evaluation of the results was done through a ROC curve-based approach.

```{r}
set.seed(123)

# Create reusable trainControl
myControl <- trainControl(
  summaryFunction = twoClassSummary,
  classProbs = TRUE, 
  verboseIter = FALSE,
  savePredictions = TRUE,
  index = createFolds(train_set$group, k = 5)
)
```

#### Logistic regression

Train on the original imbalance dataset.

```{r warning=FALSE}
# train logistic model
model_logis <- train(
    x = train_set[, -13],
    y = train_set$group,
    metric = "ROC",
    method = "glm",
    trControl = myControl,
    preProcess = c("center", "scale")
)

model_logis
```

Train by subsampling methods to address the imbalance issue.

```{r message=FALSE, warning=FALSE}
# down-sampling logistic model
myControl$sampling <- "down"

logis_down <- train(
    x = train_set[, -13],
    y = train_set$group,
    metric = "ROC",
    method = "glm",
    trControl = myControl,
    preProcess = c("center", "scale")
)

# up-sampling logistic model
myControl$sampling <- "up"

logis_up <- train(
    x = train_set[, -13],
    y = train_set$group,
    metric = "ROC",
    method = "glm",
    trControl = myControl,
    preProcess = c("center", "scale")
)

# smote logistic model
myControl$sampling <- "smote"

logis_smote <- train(
    x = train_set[, -13],
    y = train_set$group,
    metric = "ROC",
    method = "glm",
    trControl = myControl,
    preProcess = c("center", "scale")
)

# see the values of different models
logistic <- data.frame()
for (logis in list(model_logis, logis_down, logis_up, logis_smote)) {
    logistic <- rbind(logistic, logis$results[, 2:4])
}
cbind(logistic_model = c("original", "down", "up", "smote"), logistic)
```

Now repeat this process for random forest and SVM.

#### Random forest

```{r warning=FALSE}
set.seed(125749)

# train random forest
myControl$sampling <- NULL

model_rf <- train(
    x = train_set[, -13],
    y = train_set$group,
    metric = "ROC",
    method = "ranger",
    trControl = myControl,
    preProcess = c("center", "scale"),
    tuneGrid = expand.grid(mtry = 1:6,
                           splitrule = 'extratrees',
                           min.node.size = c(10, 15, 20, 25, 30)),
    importance = 'impurity'
)

model_rf
```

```{r warning=FALSE}
# down-sampling rf model
myControl$sampling <- "down"
set.seed(125749)
rf_down <- train(
    x = train_set[, -13],
    y = train_set$group,
    metric = "ROC",
    method = "ranger",
    trControl = myControl,
    preProcess = c("center", "scale"),
    tuneGrid = expand.grid(mtry = 1:6,
                           splitrule = 'extratrees',
                           min.node.size = c(10, 15, 20, 25, 30)),
    importance = 'impurity'
)

# up-sampling rf model
myControl$sampling <- "up"
set.seed(125749)
rf_up <- train(
    x = train_set[, -13],
    y = train_set$group,
    metric = "ROC",
    method = "ranger",
    trControl = myControl,
    preProcess = c("center", "scale"),
    tuneGrid = expand.grid(mtry = 1:6,
                           splitrule = 'extratrees',
                           min.node.size = c(10, 15, 20, 25, 30)),
    importance = 'impurity'
)

# smote rf model
myControl$sampling <- "smote"
set.seed(125749)
rf_smote <- train(
    x = train_set[, -13],
    y = train_set$group,
    metric = "ROC",
    method = "ranger",
    trControl = myControl,
    preProcess = c("center", "scale"),
    tuneGrid = expand.grid(mtry = 1:6,
                           splitrule = 'extratrees',
                           min.node.size = c(10, 15, 20, 25, 30)),
    importance = 'impurity'
)

# see the values of different models
rf <- data.frame()
for (rf_mod in list(model_rf, rf_down, rf_up, rf_smote)) {
    rf <- rbind(rf, rf_mod$results[which.max(rf_mod$results$ROC), c(1, 3:6)])
}
cbind(rf_model = c("original", "down", "up", "smote"), rf)
```

#### SVM

```{r warning=FALSE}
# train linear svm
myControl$sampling <- NULL

model_svm <- train(
    # change to formula, avoiding kernlab class probability calculations failed
    group ~ .,        
    data = train_set,
    metric = "ROC",
    method = "svmLinear",
    trControl = myControl,
    preProcess = c("center", "scale"),
    tuneGrid = expand.grid(C = seq(1, 5, length = 20))
)

model_svm
```

```{r}
# down-sampling svm model
myControl$sampling <- "down"

svm_down <- train(
    group ~ .,        
    data = train_set,
    metric = "ROC",
    method = "svmLinear",
    trControl = myControl,
    preProcess = c("center", "scale"),
    tuneGrid = expand.grid(C = seq(1, 5, length = 20))
)

# up-sampling svm model
myControl$sampling <- "up"

svm_up <- train(
    group ~ .,        
    data = train_set,
    metric = "ROC",
    method = "svmLinear",
    trControl = myControl,
    preProcess = c("center", "scale"),
    tuneGrid = expand.grid(C = seq(1, 5, length = 20))
)

# smote svm model
myControl$sampling <- "smote"

svm_smote <- train(
    group ~ .,        
    data = train_set,
    metric = "ROC",
    method = "svmLinear",
    trControl = myControl,
    preProcess = c("center", "scale"),
    tuneGrid = expand.grid(C = seq(1, 5, length = 20))
)

# see the values of different models
svm <- data.frame()
for (svm_mod in list(model_svm, svm_down, svm_up, svm_smote)) {
    svm <- rbind(svm, svm_mod$results[which.max(svm_mod$results$ROC), 1:4])
}
cbind(svm_model = c("original", "down", "up", "smote"), svm)
```

```{r eval=FALSE, include=FALSE}
# Predict on training set.
# Create model_list
model_list <- list(logistic = model_logis, rf = model_rf, svm = model_svm)

# predict on training set to see if it's overfitting later
for (i in 1:length(model_list)) {
    print(confusionMatrix(predict(model_list[[i]], train_set),
                          train_set$group))
}
```

### Validation

#### Metrics

-   AUC: Area Under ROC curve

-   Accuracy: the percentage of correctly classified condition and control samples.

-   Sensitivity: the fraction of correctly classified conditions related to all conditions.

-   Specificity: the fraction of controls correctly classified as controls.

-   Balanced Accuracy: the arithmetic mean of sensitivity and specificity, which is particularly useful when the two classes are imbalanced.

-   PPV: the proportion of true condition samples.

-   NPV: the proportion of true control samples.

```{r message=FALSE}
# create model_list
model_list <- list(model_logis, logis_down, logis_up, logis_smote, model_rf, rf_down, rf_up, rf_smote, model_svm, svm_down, svm_up, svm_smote)

# gather metrics together
acc <- c()
bal <- c()
sen <- c()
spe <- c()
ppv <- c()
npv <- c()
auc <-c()

for (num in 1:length(model_list)) {
    prediction <- predict(model_list[[num]], test_set)
    confus <- confusionMatrix(prediction, test_set$group)
    acc <- acc %>% append(confus$overall[["Accuracy"]])
    bal <- bal %>% append(confus$byClass[["Balanced Accuracy"]])
    sen <- sen %>% append(confus$byClass[["Sensitivity"]])
    spe <- spe %>% append(confus$byClass[["Specificity"]])
    ppv <- ppv %>% append(confus$byClass[["Pos Pred Value"]])
    npv <- npv %>% append(confus$byClass[["Neg Pred Value"]])
    ROC <- pROC::roc(test_set$group, predict(model_list[[num]], test_set, type = "prob")[,"condition"])
    auc <- auc %>% append(pROC::auc(ROC))
}

metric <- data.frame(Model = c("Logistic", "Logis_down", "Logis_up", "Logis_smote",
                               "RF", "RF_down", "RF_up", "RF_smote",
                               "SVM", "SVM_down", "SVM_up", "SVM_smote"),
                     Accuracy = acc,
                     Balance_ACC = bal, 
                     Sensitivity = sen,
                     Specificity = spe,
                     PPV = ppv,
                     NPV = npv,
                     AUC = auc) %>% 
    mutate(across(-1, ~ round(.x, 3)))

# print in order
metric %>%
    arrange(desc(Sensitivity), desc(AUC), desc(Accuracy), desc(Specificity))
```

```{r}
# plot sensitivity vs models
ggplot(metric[, c(1, 4)], aes(Sensitivity, reorder(Model, Sensitivity))) +
    geom_point() +
    geom_segment(aes(xend = 0.55, yend = reorder(Model, Sensitivity))) +
    labs(y = "Models")
```

```{r}
# plot auc vs models
ggplot(metric[, c(1, 8)], aes(AUC, reorder(Model, AUC))) +
    geom_point() +
    geom_segment(aes(xend = 0.7, yend = reorder(Model, AUC))) +
    labs(y = "Models")
```

We found that different metrics have different results for the model's performance. If it's based on AUC, then the up-sampling logistic model is the best. But if it's based on sensitivity, then SMOTE random forest is the best. We decided to use sensitivity to evaluate model performance because, in the context of clinical diagnosis, the ability to correctly classify condition cases would be a priority concern.

#### Variable importance

Let's see the variable importance of the SMOTE random forest model.

```{r}
# visualize rf
vip(rf_smote, num_features = 12)
```

According to the figure, the top five important features in the SMOTE random forest model were the median, mean, quantile 95, zero proportion, and skewness of daily activity.

## Conclusion

In conclusion, our investigation has demonstrated the potential of a number of machine learning algorithms to distinguish between depressed and healthy participants in time series of motor activity. Furthermore, SMOTE random forest outperformed other methods in our analysis for the classification task based on the sensitivity metric. Additionally, the extracted statistical features indicated that the information they include describes the primary aspects of a participant's daily activities, making it possible to distinguish between depressed and healthy participants. Nevertheless, we must integrate this kind of index with other indicators (e.g., clinical assessments) to get the big picture if we would like to implement it to aid in clinical diagnosis.
