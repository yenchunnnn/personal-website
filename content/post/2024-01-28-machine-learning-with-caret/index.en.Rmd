---
title: Model selection in the caret package
author: Yen Chun Chen
date: '2024-01-28'
slug: machine-learning-with-caret
categories:
  - R
  - Mechine Learning
tags:
  - Data Science
  - R Programming
subtitle: ''
summary: 'Comparing the performance of glmnet and gradient boosting with a real-world customer churn dataset.'
authors: []
lastmod: '2024-02-08T16:29:18+08:00'
featured: no
image:
  caption: 'Friends scenes from Reddit'
  focal_point: 'smart'
  preview_only: no
projects: []
output:
  blogdown::html_page:
    toc: true
---

**Mission**

-   Data: customer churn at telecom company

-   Task: predict which customers will churn

**Approach**

-   Fit different models and choose the best

-   Models must use the same training/test splits: `createFolds()`

-   Create a shared `trainControl` object

## Load dataframe

```{r message=FALSE, warning=FALSE}
# library package
library(tidyverse)
library(caret)
```

```{r include=FALSE}
# load data
churn <- load("C:/Users/User/Documents/R Scripts/DataCamp/data/Churn.RData")
```

```{r}
# view training data
glimpse(churn_x)
```

```{r}
# see DV
str(churn_y)
```

## Create a reusable trainControl

Use `createFolds()` to create 5 CV folds on target variable. And then Pass `myFold` to `index` to create a reusable `trainControl` for model comparison.

```{r message=FALSE, warning=FALSE}
# Create custom indices
set.seed(12556)
myFolds <- createFolds(churn_y, k = 5)

# Create reusable trainControl
myControl <- trainControl(
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  verboseIter = FALSE,
  savePredictions = TRUE,
  index = myFolds    # reuse to compare model
)
```

## Fit baseline model

### glmnet model

-   Linear model with built-in variable selection

-   Advantages

    -   Fits quickly

    -   Ignores noisy variables

    -   Provides interpretable coefficients (alpha & lambda)

```{r message=FALSE, warning=FALSE}
# Fit glmnet model
model_glmnet <- train(
  x = churn_x, 
  y = churn_y,
  metric = "ROC",
  method = "glmnet",
  trControl = myControl,
  tuneLength = 5
)

model_glmnet
```

### Visualize results

```{r}
# result model
ggplot(model_glmnet)
```

## Fit comparison model

### Gradient boosting

-   A powerful ensemble techniques that combines several weak learners into strong learners.

-   Each new model is trained to minimize the error of the previous model using gradient descent (so they're sequentially).

-   Usually get a higher performance than glmnet.

```{r message=FALSE, warning=FALSE}
set.seed(234)

# Fit gradient boosting
model_sgb <- train(
  x = churn_x, 
  y = churn_y,
  metric = "ROC",
  method = "gbm",
  trControl = myControl,
  tuneGrid = expand.grid(interaction.depth = 3:8,
                         shrinkage = 0.15,
                         n.trees = c(150, 200, 250),
                         n.minobsinnode = 2:3),
  verbose = F
)

model_sgb
```

### Visualize results

```{r}
# visualize gradient boosting
ggplot(model_sgb)
```

## Comparing models

Selection criteria:

-   Highest average ROC

-   Lowest standard deviation in ROC

### Create a resamples object

Let's evaluate their out-of-sample predictions and see which model performs best with the dataset.

`resamples()` function takes a list of models as input and can be used to compare dozens of models at once, but you must ensure that each model has the same training data --- that is, the same CV folds --- and `trainControl`.

```{r}
# Create model_list
model_list <- list(glmnet = model_glmnet, sgb = model_sgb)

# Pass model_list to resamples()
resamples <- resamples(model_list)

# Summarize the results
summary(resamples)
```

### Plot resamples

**box-and-whisker plot**

A higher median (black dot) of the ROC as well as a smaller range between the min and max of the ROC are preferred.


```{r}
# Create bwplot
bwplot(resamples, metric = "ROC")
```

Gradient boosting has higher median and smaller range of ROC, so it's better than glmnet.

**Scatter plot**

A scatter plot can help us determine whether one model outperforms the other across all folds.

```{r}
# Create xyplot
xyplot(resamples, metric = "ROC")
```

There's one fold of data where glmnet is better than gradient boosting.


### Ensembling models

Another approach is using the `caretEnsemble` package to construct a list of caret models (`caretList`), which will use the same resampling folds too.


```{r message=FALSE, warning=FALSE}
library(caretEnsemble)

set.seed(234)

# creat caretList object
model_carlist <- caretList(
    x = churn_x,
    y = churn_y,
    metric="ROC",
    trControl = myControl,
    tuneList = list(
        glmnet = caretModelSpec(method = "glmnet", tuneLength = 5),
        gbm = caretModelSpec(method = "gbm", 
                             tuneGrid = expand.grid(interaction.depth = 3:8,
                                        shrinkage = 0.15,
                                        n.trees = c(150, 200, 250),
                                        n.minobsinnode = 2:3),
                             verbose = F)
    )
)

model_carlist
```

You can see that the attrition class of the object was `caretList`.

Now let's use logistic regression to ensemble the two models.

```{r}
# Create ensemble model
stack <- caretStack(model_carlist, method = "glm")

# Look at summary
summary(stack)
```

Finally, based on the plots and statistical results of comparing models, gradient boosting performs marginally better than the glmnet model on this churn data.

*Notice: The dataset is from DataCamp's exercise.*
